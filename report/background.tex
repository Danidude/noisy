\chapter{Background}
\label{ch:background}

\section{The multi-armed bandit problem}
A popular and surprisingly fitting way to model stochastic optimization problems 
is through the multi-armed bandit scenario. Imagine, if you will, the iconic 
gambling machine to be found in any casino. Only this apparatus has not one, but 
many arms with which a player may try her luck. Oh, but which arm to choose? 
With so many possible choices, a strategy for optimising the machine’s payout 
must be employed. 

Before any arm-pulling has been done, nothing is known about the possible 
payoffs from the multitude of available arms exept that some arms are better 
than others -- obviously our player will profit the most from playing only the 
best arm. And so, it comes to this: she must make a tradeoff between exploration 
-- discovering which arm is truly the best one -- and exploitation -- actually 
picking the best arm.

More specifically, in this paper we look at Gaussian bandits.
This means that arms are normally distributed, and each arm is described by a mean and standard deviation.
Furthermore, seeing as the normal distribution is self-conjugate, this allows for the use of normal distributions to describe the arm estimates used in the LTS algorithm.





\section{Learning automata}
The model of a learning automaton is illustrated in figure~\ref{fig:la}.
\begin{figure}[ht]
        \begin{verbatim}
                   |-------------|  r
              |--->| Environment |----|
              |    |-------------|    |
              |                       | 
              | a  |-------------|    |
              |----|   Learner   |<---|
                   |-------------|
         
                     a: action
                     r: reward
        \end{verbatim}
        \label{fig:la}
    \caption{Replace with drawing}
\end{figure}

\section{Thompson sampling}
Thompson sampling is a technique for updating prior knowledge with new 
information.

It employs the nefarious and infamous parameter known as the Observation Noise, 
or $\sigma_{ob}$. This is not for the faint of heart!


\section{The Goore Game}
Introduced by Tsetlin in 1973, the Goore game is a cooperation game where each  
player is not privvy to information on the other players choices. It can be 
described as a voting process where the referree knows the correct answer, and 
the referrers cast their votes as simple ‘yes’ or ‘no’. The correct answer is a 
certain number of yes’s and no’s, and players are rewarded based on how close 
they collectively are to the ideal solution.



\section{Gaussian process}
A secretive procedure invented by a certain Mr. G. Process. Contrary to 
widespread belief, it does not involve the use of black magick.


\section{Implementation}

\section{Python and Pypy}
The goore game problem was solved using python code, since python is easy to write
and is a powerfull and fast running. Python in it self was a bit to slow, so we used
Pypy. Pypy is a reimplementation of Python that focus more on speed, making running
simulations take less time.

\subsection{Haskell}

\section{Algorithms for solving the multi-armed bandit problem}

In addition to local Thompson sampling, we implemented the UCB1 algorithm for 
comparing optimised Thompson samplers. Both algorithms are described in the next 
subsections.

\subsection{Local Thompson sampling}
A lesser known algorithm, based on principles set forth by Thompson in 1933. 
For every available arm it keeps a distribution estimate, which is 
updated based on received reward. The use of Thompson sampling in the bandit 
setting depends on a parameter known as observation noise, $\sigma_{ob}$, which 
is assigned a value based on how much new information is to be trusted. A low 
observation noise signifies that new information should be considered 
trustworthy, while a high value means that observations are insecure. 

\subsection{UCB1-Tuned}


UCB1 \cite{Auer02UCB1} (Upper Confidence Bound) is a well-known algorithm for multi-armed bandits.
It is popular due to being very simple and reasonably performant. In comparison 
to LTS, it does not require any constant parameters, and in place of a given 
start estimate for the available arms the fist $K$ rounds are dedicated to 
initialising the arm estimates by playing every arm once.

UCB1-Tuned is a version of UCB1 that takes into consideration the estimate variance in addition to the estimate mean. This should cause it to work better in practice.

The UCB1-Tuned strategy is simply to play the arm with index $i$ such that
\begin{displaymath}
    i = \operatorname*{argmax}_{j \in \{ 1..K \}} \left(\hat{\mu}_j + 
    \sqrt{\frac{\ln{n}}{n_j} \min(\frac{1}{4},\hat{V}_j)}\right)
\end{displaymath}
where 
\begin{displaymath}
    \hat{V}_j = \hat{\sigma}_j^2 + \sqrt{\frac{2\ln{n}}{n_j}}\text{.}
\end{displaymath}

$K$ is the number of available arms, $n_j$ the numer of times arm $j$ has 
been played, $n$ is the total number of plays, and $\hat{\mu}_j 
=\frac{(\text{cumulative reward for arm j})}{n_j}$.


% \section{Sample stuff}
% Some simple and useful latex formatting.
% 
% \subsection{Quotations and citing}
% It is explained in detail in \cite[Ch.20]{Norvig03} that
% 
% \begin{quotation}
% \noindent \textit{``the true hypothesis eventually dominates the Bayesian 
% predication. For any fixed prior that does not rule out the true hypothesis, the 
% posterior probability of any false hypothesis will eventually vanish, simply 
% because the probability of generating ``uncharacteristic'' data indefinitely is 
% vanishingly small.''}
% \end{quotation}
% 
% \subsection{Figures}
% This distribution, and its probability density function, is displayed in Figure 
% \ref{fig:gaussian_distr_pdf}.
% \begin{figure}[ht]
%     \center\includegraphics[width=10cm]{images/normal_distr_pdf}	
%     \label{fig:gaussian_distr_pdf}
%     \caption{The Normal distribution PDF.}
% \end{figure}
% 
% \subsection{Equations}
% By using these probabilities, and Bayes formula, we can derive the Bayes 
% classifier.
% \begin{equation}
%     P(\omega_i | \boldsymbol{x}, \mathcal{X}) = \frac{p(\boldsymbol{x}|\omega_i, 
%     \mathcal(X))P(\omega_i|\mathcal{X})}{\sum_{j=1}^{c}p(\boldsymbol{x}|\omega_j, 
%     \mathcal{X})P(\omega_j|\mathcal{X})},
%     \label{eq:bayes_formula_1}
% \end{equation}
% 
% when we can separate the training samples by class into $c$ subsets 
% $\mathcal{X}_1, \ldots, \mathcal{X}_c$, with the samples in $\mathcal{X}_i$ 
% belonging to $\omega_i$.
% 
