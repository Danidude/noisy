\chapter{Background}
\label{ch:background}

\section{Reinforcement learning}
Given uncertian and possibly varying parameters an agent choses
its actions based on prior knowledge and by repeated trials. The very first action
will be chosen by random and the agent will then recieve a reward or
penalty from the environment. This feedback will then be used to update the expected reward
from that perticular action. By repeatedly chosing the actions and receiving feedback
from the environment, as pictured in figure~\ref{fig:la}, the agent is able to
decide which action is the most profitable. 

If the environment does in fact have varying parameters, meaning that the reward
is normally distributed, the agent have to weigh exploration against
explotation. Given two choices, if the agent choses option A and then option B one time each and
option A gives a higher reward it does not necessarily mean that option A gives the highest
reward over time. Thus the agent have to decide on wether to continue
to test both options, to get a clearer picture of which option is the better one, or to repeatedly
select the option it believes is the best one to maximize profit. 

\begin{figure}[ht]
        \begin{verbatim}
                   |-------------|  r
              |--->| Environment |----|
              |    |-------------|    |
              |                       | 
              | a  |-------------|    |
              |----|   Agent   |<---|
                   |-------------|
         
                     a: action
                     r: reward
        \end{verbatim}
        \label{fig:la}
    \caption{Replace with drawing}
\end{figure}

\section{The multi-armed bandit problem}
A popular and surprisingly fitting way to model stochastic optimization problems 
is through the multi-armed bandit scenario. Imagine, if you will, the iconic 
gambling machine to be found in any casino. Only this apparatus has not one, but 
many arms with which a player may try her luck. Oh, but which arm to choose? 
With so many possible choices, a strategy for optimising the machine’s payout 
must be employed. 

Before any arm-pulling has been done, nothing is known about the possible 
payoffs from the multitude of available arms exept that some arms are better 
than others -- obviously our player will profit the most from playing only the 
best arm. And so, it comes to this: she must make a tradeoff between exploration 
-- discovering which arm is truly the best one -- and exploitation -- actually 
picking the best arm.

More specifically, in this paper we look at Gaussian bandits.
This means that arms are normally distributed, and each arm is described by a mean and standard deviation.
Furthermore, seeing as the normal distribution is self-conjugate, this allows for the
use of normal distributions to describe the arm estimates used in the LTS algorithm.

\section{The Goore Game}
Introduced by Tsetlin in 1973, the Goore game is a cooperation game where each  
player is not privy to information on the other players choices. It can be 
described as a voting process where the referee knows the correct answer, and 
the players cast their votes as simple ‘yes’ or ‘no’. The correct answer is a 
certain number of yes’s and no’s, and players are rewarded based on how close 
they collectively are to the ideal solution.

At the start of the game all participants selects an action at random. The referee
gathers all votes and based on a unimodal performance criterion it decides on
the probability of giving each player a reward. Meaning that there is only a single
maximum value, or a single ideal target. If the ideal target for ten players is
seven 'yes' and three 'no' the reward would not be guaranteed, it would however be
optimal. 

Lets say that ten players all select 'no' at the first round. The probability of
each individual player receiving a reward would be fairly low and thus most of them
are likely to chose another action. Stil, given a low chance of reward some of them
could receive one and thus chose to stick to their previous action. Even if the players
chose the ideal solution during the first round it is likely that not all players would
receive a reward and thus it takes some time to explore all options before the players
converge on the ideal solution.

\section{Algorithms for solving the multi-armed bandit problem}
In addition to local Thompson sampling, we implemented the UCB1 algorithm for 
comparing optimised Thompson samplers. Both algorithms are described in the next 
subsections.

\subsection{Local Thompson sampling}
A lesser known algorithm, based on principles set forth by Thompson in 1933. 
For every available arm it keeps a distribution estimate, which is 
updated based on received reward. The use of Thompson sampling in the bandit 
setting depends on a parameter known as observation noise, $\sigma_{ob}$, which 
is assigned a value based on how much new information is to be trusted. A low 
observation noise signifies that new information should be considered 
trustworthy, while a high value means that observations are insecure. 

After receiving a reward $r_i$ by pulling arm $i$ the bandit is updated. The mean and variance
is calculated as follows:
\begin{displaymath}
\mu_i [t + 1] = \frac{\sigma_i^2 [t] \cdot r_i + \sigma_{ob}^2 \cdot \mu_i [t]}{\sigma_i^2 [t] + \sigma_{ob}^2}
\end{displaymath}

\begin{displaymath}
\sigma_i^2 [t + 1] = \frac{\sigma_i^2 [t] \sigma_{ob}^2}{\sigma_i^2 [t] + \sigma_{ob}^2}
\end{displaymath}
Where $\sigma_i^2 [t + 1]$ and $\mu_i [t + 1]$ is the variance and mean of the updated arm and
$\sigma_i^2 [t]$ and $\mu_i [t]$ is the variance and mean of the arm in its previous state.

\begin{figure}[htbp]
    \centering
% Haskell code to generate data:
% let ppB b = unlines $ map ppL b
% let ppL (t,a,b,c) = show t ++ " " ++ show a ++ " " ++ show b ++ " " ++ show c
% writeFile "est.txt" . unlines . map ppB . transpose 
%   $ map (\(est, ob) -> (zip4 (var2 est ob) ([1..10000]) (repeat est) (repeat ob)))
%         [(est, ob) | est <- [2500],
%                      ob <- ([0.001,0.002, 0.003, 0.006, 0.01, 0.03,0.06] ++ [0.1,0.2..3.0])]

%    Let's not recompile this thing all the time.
%    \begin{gnuplot}[terminal=epslatex,terminaloptions=color]
%    set grid
%    set ylabel "[r]{\\shortstack{Observation noise \\\\ ($\\sigma_{ob}^2$)}}" offset 6,-2
%    set xlabel 'Rounds' offset 0,-0.5
%    set zlabel 'Variance estimate ($\hat{\sigma}^2$)' rotate offset -0.5,0
%    set xyplane 0
%    set log z; set log cb
%    set xrange [0:1000]
%    set zrange [1e-06:3]
%    set ytics 0.4 offset 0,-0.5
%    set xtics offset 0,-0.5
%    set palette rgbformulae 23,28,3
%    splot 'est.txt' using 2:(column(4) < 1.5 ? column(4) : 1/0):(column(3) == 2500.0 ? column(1) : 1/0) every :::::1000 with pm3d title "" \
%    ,'est.txt' using 2:(column(4)<1.5?column(4):1/0):(column(3)==2500.0?column(1):1/0) every 2:100::50::1001 with line lt -1 lw 1 title "" \
%    ,'est.txt'       using 2:(column(4) < 1.5 ? column(4) : 1/0):(column(3) == 2500.0 ? column(1) : 1/0) every 2:10::::70  with line lt -1 lw 1 title ""
%    \end{gnuplot}
    \input{ob-var-fig1.tex}
\caption{Variance}
\label{fig:variance}
\end{figure}

\subsection{UCB1-Tuned}
UCB1 \cite{Auer02UCB1} (Upper Confidence Bound) is a well-known algorithm for multi-armed bandits.
It is popular due to being very simple and reasonably performant. In comparison 
to LTS, it does not require any constant parameters, and in place of a given 
start estimate for the available arms the fist $K$ rounds are dedicated to 
initialising the arm estimates by playing every arm once.

UCB1-Tuned is a version of UCB1 that takes into consideration the estimate variance in addition to the estimate mean. This should cause it to work better in practice.

The UCB1-Tuned strategy is simply to play the arm with index $i$ such that
\begin{displaymath}
    i = \operatorname*{argmax}_{j \in \{ 1..K \}} \left(\hat{\mu}_j + 
    \sqrt{\frac{\ln{n}}{n_j} \min(\frac{1}{4},\hat{V}_j)}\right)
\end{displaymath}
where 
\begin{displaymath}
    \hat{V}_j = \hat{\sigma}_j^2 + \sqrt{\frac{2\ln{n}}{n_j}}\text{.}
\end{displaymath}

$K$ is the number of available arms, $n_j$ the numer of times arm $j$ has 
been played, $n$ is the total number of plays, and $\hat{\mu}_j 
=\frac{(\text{cumulative reward for arm j})}{n_j}$.


% \section{Sample stuff}
% Some simple and useful latex formatting.
% 
% \subsection{Quotations and citing}
% It is explained in detail in \cite[Ch.20]{Norvig03} that
% 
% \begin{quotation}
% \noindent \textit{``the true hypothesis eventually dominates the Bayesian 
% predication. For any fixed prior that does not rule out the true hypothesis, the 
% posterior probability of any false hypothesis will eventually vanish, simply 
% because the probability of generating ``uncharacteristic'' data indefinitely is 
% vanishingly small.''}
% \end{quotation}
% 
% \subsection{Figures}
% This distribution, and its probability density function, is displayed in Figure 
% \ref{fig:gaussian_distr_pdf}.
% \begin{figure}[ht]
%     \center\includegraphics[width=10cm]{images/normal_distr_pdf}	
%     \label{fig:gaussian_distr_pdf}
%     \caption{The Normal distribution PDF.}
% \end{figure}
% 
% \subsection{Equations}
% By using these probabilities, and Bayes formula, we can derive the Bayes 
% classifier.
% \begin{equation}
%     P(\omega_i | \boldsymbol{x}, \mathcal{X}) = \frac{p(\boldsymbol{x}|\omega_i, 
%     \mathcal(X))P(\omega_i|\mathcal{X})}{\sum_{j=1}^{c}p(\boldsymbol{x}|\omega_j, 
%     \mathcal{X})P(\omega_j|\mathcal{X})},
%     \label{eq:bayes_formula_1}
% \end{equation}
% 
% when we can separate the training samples by class into $c$ subsets 
% $\mathcal{X}_1, \ldots, \mathcal{X}_c$, with the samples in $\mathcal{X}_i$ 
% belonging to $\omega_i$.
% 
