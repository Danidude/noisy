\chapter{Solution}
\label{ch:solution}

In this chapter we describe the general setup for experiments, first for the multi-armed bandit and then for the Goore game.

\section{Multi-armed bandit}
The multi-armed bandits in our experiments have one uniquely best arm, and all other arms are identically distributed with an expected reward which is worse than the expected reward from the best arm.
The parameters which fully define the multi-armed bandit scenarios are as follows:
\begin{itemize}
    \item Mean of the best arm ($\mu^*$)
    \item Mean of the bad arms ($\mu$)
    \item Standard deviation of all arms ($\sigma$)
    \item Number of arms ($K$)
    \item Number of rounds, time steps, or horizon ($T$)
\end{itemize}

In addition, for LTS a starting estimate for the arms must be provided.
Here the mean value is set to 0 and the standard deviation is set to 50, following the recommendation in \cite{Glimsdal12}.

The mean value for the best arm is for the most part set to 5.0, and all arm parameters stay between 0.0 and 10.0.
Note that as $\mu \to \mu^*$, the penalty for choosing the wrong arm decreases.
Furthermore, $\sigma$ determines the noisiness of rewards.

The maximum number of arms we test is 64.
This is a reasonably high number which represents a difficult task compared to the minimum of two arms.
We do not elect to show results for when more arms are present as the rate of change in the results decrease rapidly with the number of arms.

We calculate the best observation noise for a range of different values for $T$ up to 1000, as most interesting effects seem to wane around this point.

We developed a couple of ways to compute and visualise the needed data:
First, a brute force method calculating cumulative rewards for all given observation noises, and second, a method that, given the simulation parameters, employs a multi-armed bandit approach for finding the best observation noise.

\subsection{Brute force exploration}
By testing a set of values in a given range it is possible to approximately determine which value yields the maximum.
The exactness of the result is then determined by how fine-grained the tested values are, and by how many runs the results are averaged over.
In our case, we elected for a step size of 0.01, and the values where averaged over up to 100,000 runs.

Although this method is crude, it is helpful for gaining an indication towards what values are best or what values need more testing.
It also gives us the opportunity to have results of arbitrary precision, where an increased precision is achieved by increasing the number of runs and decreasing the step size.
The downside of this solution is that many tests and much computation may be required, and most of the results generated are not interesting as we are only interested in the value yielding the maximum.

\subsection{Bandit approach}
As the multi-armed bandit approach can be used to find the maximum of a stochastic function, it fits perfectly to the task of finding the best observation noise for a multi-armed bandit employing the LTS strategy.
Like in the brute-force method, a range of observation noise values is subdivided, and each of these values count as an arm of a multi-armed bandit.
The player then selects one of these arms according to its strategy.
The environment proceeds to determine the reward by using the \ob{} associated with the selected arm in an appropriate number of bandit simulation, and returns the average total cumulative reward to the player, which continues to iterate as in a normal multi-armed bandit scenario.
The reason for averaging the cumulative rewards is to reduce the variance of the rewards.

\section{Goore Game}

Contrary to in the multi-armed bandit scenario there are no predetermined, universally best arm in the Goore game.
The reward distribution is determined dynamically at each iteration of the game given the votes of the players together with the performance criterion $G(\lambda)$.
Now we describe the parameters which define a Goore game scenario.

\begin{itemize}
    \item Number of players ($N$)
    \item Ratio, or the fraction of yes-votes ($\lambda^*$)
    \item Added Gaussian white noise ($\sigma_n$)
    \item Number of rounds ($T$)
\end{itemize}

In addition, LTS needs starting estimate for the two arms.
These were set to the normal distribution $N(3.5,3.0)$.
The start values for mean and standard deviation of the arms are fairly insignificant as long as the mean or the standard deviation is high enough to ensure that each arm is chosen at least once.

We test $N$ between 2 and 40 to find how the increase in players affects \obstar{}.
The ratios used for the performance function are 0.2, 0.3, and 0.4.
Some ratios should present harder problem for certain numbers of players, as it is impossible for, say, 7 players to reach the exact ratio of 0.4 yes-votes.
We only use ratios below 0.5, since the corresponding ratios above 0.5 would provide identical results.
For instance, the ratio 0.8 corresponds to the ratio 0.2.
The standard deviation of the Gaussian distribution used for the performance criterion is set at 0.1 throughout all experiments.
Finally by varying the added Gaussian white noise we simulate different strengths of noise in the feedback from the environment.

For the Goore game only a brute force approach was used to find the best observation noise.


%By testing all the values in a given range to determine where the maximum lies under different
%circumstances. A downside to this menthod is that it will generate some use full data and some not so
%use full data. For example, on a ratio 50/50, with 5 players is it hard for the  odd player to 
%determine whether yes or no is the best answer. However this method do gives some use full inn-site on
%how Goore Game works under strange circumstances that are nod ideal. As this solution is easy to set
%up and start, it is however needed to take many more tests and more time.


% \section{Proposed solution / algorithm}
% 
% \subsection{The basic algorithm}
% 
% \subsection{Discussion of design issues}
% 
% 
% \subsection{Algorithmic Enhancements}
% 
% 
% \subsection{Discussion of the Parameter Space}
% 
% 
% \section{Prototype}
% 
% \section{Justification of Claim to Originality}
% 
% \section{Valuation of Contribution}
% 
% \section{Alternatives}
