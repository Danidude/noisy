\chapter{Proposed Solution}
\label{ch:solution}
Approx. 10 pages

\section{Multi-armed bandit}

The multi-armed bandits in our trials have one uniquely best arm, and all other arms are identically distributed.
Following is a definition of parameters used for the multi-armed bandits.
\begin{itemize}
    \item Mean of the best arm
    \item Standard deviation of the best arm
    \item Mean of the bad arms
    \item Standard deviation of the bad arms
    \item Estimate mean
    \item Estimate standard deviation
    \item Number of arms
    \item Observation noise
\end{itemize}
All parameters for normal distributions are kept between $0.0$ and $10.0$.

\subsection{Solution strategy}

We developed a couple of ways to compute and visualise the needed data.
A brute force method calculating cumulative rewards for all given observation noises, and a method that, given the simulation parameters, employs a bandit approach to calculating the best observation noise.

In addition we calculate instant rewards at log-scale timesteps given the simulation parameters, for verifying that the solution found is in fact the best one.

\subsubsection{Brute force solution}

By testing all the values in a given range it is possible to determine where the maximum lies.
Although this method is crude, it is helpful for gaining an indication towards what values are best or what values need more testing.
The downside of this solution is that many tests and much computation may be required, and most of the results generated are not interesting if we in reality are only looking for a (unique) maximum. 

\subsubsection{Bandit approach}

As the multi-armed bandit approach can be used to find the maximum of a
stochastic function, it fits perfectly to the task of finding the best
observation noise for a multi-armed bandit employing the local Thompson
sampling strategy.

\subsubsection{Measuring performance}

There are several ways to keep score and look at data from a bandit simulation.
First, one can look at the cumulative reward recieved by the player.
Second, there is the instant rewards, that is, the reward received by the player at a given time $t$.
By logging this one can easily see how the player increases its performance over time.
Third, a much used approach is to calculate regret.
In fact, the multi-armed bandit problem is often stated as a minimisation problem with respect to regret.
Last, the number of times the best arm is selected at any given time can be an interesting metric.


\section{Goore Game}



% \section{Proposed solution / algorithm}
% 
% \subsection{The basic algorithm}
% 
% \subsection{Discussion of design issues}
% 
% 
% \subsection{Algorithmic Enhancements}
% 
% 
% \subsection{Discussion of the Parameter Space}
% 
% 
% \section{Prototype}
% 
% \section{Justification of Claim to Originality}
% 
% \section{Valuation of Contribution}
% 
% \section{Alternatives}
